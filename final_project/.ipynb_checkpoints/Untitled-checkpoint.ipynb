{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "548dbc57-a75e-4a6f-807d-1d77a824ff00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "from functools import partial\n",
    "from timeit import default_timer as timer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0727394c-f03c-4528-9d99-a586d73df935",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def g(x):\n",
    "\t\"\"\" sigmoid function \"\"\"\n",
    "\treturn 1.0 / (1.0 + np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b11e8977-a8d7-4561-803b-5f68aaf2a571",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grad_g(x):\n",
    "\t\"\"\" gradient of sigmoid function \"\"\"\n",
    "\tgx = g(x)\n",
    "\treturn gx * (1.0 - gx)\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b454e47-0470-4921-8f0e-53606fcbc52b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(Theta1, Theta2, X):\n",
    "\t\"\"\" Predict labels in a trained three layer classification network.\n",
    "\tInput:\n",
    "\t  Theta1       trained weights applied to 1st layer (hidden_layer_size x input_layer_size+1)\n",
    "\t  Theta2       trained weights applied to 2nd layer (num_labels x hidden_layer_size+1)\n",
    "\t  X            matrix of training data      (m x input_layer_size)\n",
    "\tOutput:     \n",
    "\t  prediction   label prediction\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tm = np.shape(X)[0]                    # number of training values\n",
    "\tnum_labels = np.shape(Theta2)[0]\n",
    "\t\n",
    "\ta1 = np.hstack((np.ones((m,1)), X))   # add bias (input layer)\n",
    "\ta2 = g(a1 @ Theta1.T)                 # apply sigmoid: input layer --> hidden layer\n",
    "\ta2 = np.hstack((np.ones((m,1)), a2))  # add bias (hidden layer)\n",
    "\ta3 = g(a2 @ Theta2.T)                 # apply sigmoid: hidden layer --> output layer\n",
    "\t\n",
    "\tprediction = np.argmax(a3,1).reshape((m,1))\n",
    "\treturn prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a63d4a67-3a8b-4d17-a5a6-694af6c44178",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reshape(theta, input_layer_size, hidden_layer_size, num_labels):\n",
    "\t\"\"\" reshape theta into Theta1 and Theta2, the weights of our neural network \"\"\"\n",
    "\tncut = hidden_layer_size * (input_layer_size+1)\n",
    "\tTheta1 = theta[0:ncut].reshape(hidden_layer_size, input_layer_size+1)\n",
    "\tTheta2 = theta[ncut:].reshape(num_labels, hidden_layer_size+1)\n",
    "\treturn Theta1, Theta2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ebafa2e-f7c9-4324-8fda-90f8a8ff2af5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:29\u001b[0;36m\u001b[0m\n\u001b[0;31m    a = 1\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def cost_function(theta, input_layer_size, hidden_layer_size, num_labels, X, y, lmbda):\n",
    "\t\"\"\" Neural net cost function for a three layer classification network.\n",
    "\tInput:\n",
    "\t  theta               flattened vector of neural net model parameters\n",
    "\t  input_layer_size    size of input layer\n",
    "\t  hidden_layer_size   size of hidden layer\n",
    "\t  num_labels          number of labels\n",
    "\t  X                   matrix of training data\n",
    "\t  y                   vector of training labels\n",
    "\t  lmbda               regularization term\n",
    "\tOutput:\n",
    "\t  J                   cost function\n",
    "\t\"\"\"\n",
    "    \n",
    "\tglobal theta_v = []\n",
    "\tglobal N_iter\n",
    "\t# unflatten theta\n",
    "\tTheta1, Theta2 = reshape(theta, input_layer_size, hidden_layer_size, num_labels)\n",
    "\t   \n",
    "\tif N_iter % 10 == 0:\n",
    "        \n",
    "\t\n",
    "\t# number of training values\n",
    "\tm = len(y)\n",
    "\t\n",
    "\t# Feedforward: calculate the cost function J:\n",
    "\t\n",
    "\ta1 = np.hstack((np.ones((m,1)), X))   \n",
    "\ta2 = g(a1 @ Theta1.T)                 \n",
    "\ta2 = np.hstack((np.ones((m,1)), a2))  \n",
    "\ta3 = g(a2 @ Theta2.T)                 \n",
    "\t\n",
    "\ty_mtx = 1.*(y==0)\n",
    "\tfor k in range(1,num_labels):\n",
    "\t\ty_mtx = np.hstack((y_mtx, 1.*(y==k)))\n",
    "\n",
    "\t# cost function\n",
    "\tJ = np.sum( -y_mtx * np.log(a3) - (1.0-y_mtx) * np.log(1.0-a3) ) / m\n",
    "\n",
    "\t# add regularization\n",
    "\tJ += lmbda/(2.*m) * (np.sum(Theta1[:,1:]**2)  + np.sum(Theta2[:,1:]**2))\n",
    "        \n",
    "\tN_iter += 1\n",
    "    \n",
    "\treturn J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71544329-95af-40ff-8181-95f57c57da3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient(theta, input_layer_size, hidden_layer_size, num_labels, X, y, lmbda):\n",
    "\t\"\"\" Neural net cost function gradient for a three layer classification network.\n",
    "\tInput:\n",
    "\t  theta               flattened vector of neural net model parameters\n",
    "\t  input_layer_size    size of input layer\n",
    "\t  hidden_layer_size   size of hidden layer\n",
    "\t  num_labels          number of labels\n",
    "\t  X                   matrix of training data\n",
    "\t  y                   vector of training labels\n",
    "\t  lmbda               regularization term\n",
    "\tOutput:\n",
    "\t  grad                flattened vector of derivatives of the neural network \n",
    "\t\"\"\"\n",
    "\t\n",
    "\t# unflatten theta\n",
    "\tTheta1, Theta2 = reshape(theta, input_layer_size, hidden_layer_size, num_labels)\n",
    "\t\n",
    "\t# number of training values\n",
    "\tm = len(y)\n",
    "\t\n",
    "\t# Backpropagation: calculate the gradients Theta1_grad and Theta2_grad:\n",
    "\t\n",
    "\tDelta1 = np.zeros((hidden_layer_size,input_layer_size+1))\n",
    "\tDelta2 = np.zeros((num_labels,hidden_layer_size+1))\n",
    "\n",
    "\tfor t in range(m):\n",
    "\t\t\n",
    "\t\t# forward\n",
    "\t\ta1 = X[t,:].reshape((input_layer_size,1))\n",
    "\t\ta1 = np.vstack((1, a1))   #  +bias\n",
    "\t\tz2 = Theta1 @ a1\n",
    "\t\ta2 = g(z2)\n",
    "\t\ta2 = np.vstack((1, a2))   #  +bias\n",
    "\t\ta3 = g(Theta2 @ a2)\n",
    "\t\t\n",
    "\t\t# compute error for layer 3\n",
    "\t\ty_k = np.zeros((num_labels,1))\n",
    "\t\ty_k[y[t,0].astype(int)] = 1\n",
    "\t\tdelta3 = a3 - y_k\n",
    "\t\tDelta2 += (delta3 @ a2.T)\n",
    "\t\t\n",
    "\t\t# compute error for layer 2\n",
    "\t\tdelta2 = (Theta2[:,1:].T @ delta3) * grad_g(z2)\n",
    "\t\tDelta1 += (delta2 @ a1.T)\t\n",
    "\n",
    "\tTheta1_grad = Delta1 / m\n",
    "\tTheta2_grad = Delta2 / m\n",
    "\n",
    "\t# add regularization\n",
    "\tTheta1_grad[:,1:] += (lmbda/m) * Theta1[:,1:]\t\n",
    "\tTheta2_grad[:,1:] += (lmbda/m) * Theta2[:,1:]\n",
    "\n",
    "\t# flatten gradients\n",
    "\tgrad = np.concatenate((Theta1_grad.flatten(), Theta2_grad.flatten()))\n",
    "\n",
    "\treturn grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45ab1771-5b32-47d8-9b6f-6d6697d4d622",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_iter = 1\n",
    "J_min = np.Inf\n",
    "theta_best = []\n",
    "Js_train = np.array([])\n",
    "Js_test = np.array([])\n",
    "\n",
    "theta_v = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe6e1575-e4b3-40b5-bb1c-7ca716043ecc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def callbackF(input_layer_size, hidden_layer_size, num_labels, X, y, lmbda, test, test_label, theta_k):\n",
    "\t\"\"\" Calculate some stats per iteration and update plot \"\"\"\n",
    "\tglobal N_iter\n",
    "\tglobal J_min\n",
    "\tglobal theta_best\n",
    "\tglobal Js_train\n",
    "\tglobal Js_test\n",
    "\t# unflatten theta\n",
    "\tTheta1, Theta2 = reshape(theta_k, input_layer_size, hidden_layer_size, num_labels)\n",
    "\t# training data stats\n",
    "\tJ = cost_function(theta_k, input_layer_size, hidden_layer_size, num_labels, X, y, lmbda)\n",
    "\ty_pred = predict(Theta1, Theta2, X)\n",
    "\taccuracy = np.sum(1.*(y_pred==y))/len(y)\n",
    "\tJs_train = np.append(Js_train, J)\n",
    "\t# test data stats\n",
    "\tJ_test = cost_function(theta_k, input_layer_size, hidden_layer_size, num_labels, test, test_label, lmbda)\n",
    "\ttest_pred = predict(Theta1, Theta2, test)\n",
    "\taccuracy_test = np.sum(1.*(test_pred==test_label))/len(test_label)\n",
    "\tJs_test= np.append(Js_test, J_test)\n",
    "\t# print stats\n",
    "\tprint('iter={:3d}:  Jtrain= {:0.4f} acc= {:0.2f}%  |  Jtest= {:0.4f} acc= {:0.2f}%'.format(N_iter, J, 100*accuracy, J_test, 100*accuracy_test))\n",
    "\tN_iter += 1\n",
    "\t# Update theta_best\n",
    "\tif (J_test < J_min):\n",
    "\t\ttheta_best = theta_k\n",
    "\t\tJ_min = J_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19722b5b-255e-457c-9ee5-ddf839265450",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial cost function J = 2.0368341799643392\n",
      "initial accuracy on training set = 0.3333333333333333\n",
      "         Current function value: 1.908880\n",
      "         Iterations: 20\n",
      "         Function evaluations: 40\n",
      "         Gradient evaluations: 40\n",
      "accuracy on training set = 0.4066666666666667\n",
      "accuracy on test set = 0.41333333333333333\n"
     ]
    }
   ],
   "source": [
    "# set the random number generator seed\n",
    "np.random.seed(917)\n",
    "\n",
    "# Load the training and test datasets\n",
    "train = np.genfromtxt('train.csv', delimiter=',')\n",
    "test = np.genfromtxt('test.csv', delimiter=',')\n",
    "\n",
    "# get labels (0=Elliptical, 1=Spiral, 2=Irregular)\n",
    "train_label = train[:,0].reshape(len(train),1)\n",
    "test_label = test[:,0].reshape(len(test),1)\n",
    "\n",
    "# normalize image data to [0,1]\n",
    "train = train[:,1:] / 255.\n",
    "test = test[:,1:] / 255.\n",
    "\n",
    "# Construct our data matrix X (2700 x 5000)\n",
    "X = train\n",
    "\n",
    "# Construct our label vector y (2700 x 1)\n",
    "y = train_label\n",
    "\n",
    "# Two layer Neural Network parameters:\n",
    "m = np.shape(X)[0]\n",
    "input_layer_size = np.shape(X)[1]\n",
    "hidden_layer_size = 8\n",
    "num_labels = 3\n",
    "lmbda = 1.0    # regularization parameter\n",
    "\n",
    "# Initialize random weights:\n",
    "Theta1 = np.random.rand(hidden_layer_size, input_layer_size+1) * 0.4 - 0.2\n",
    "Theta2 = np.random.rand(num_labels, hidden_layer_size+1) * 0.4 - 0.2\n",
    "\n",
    "# flattened initial guess\n",
    "theta0 = np.concatenate((Theta1.flatten(), Theta2.flatten()))\n",
    "J = cost_function(theta0, input_layer_size, hidden_layer_size, num_labels, X, y, lmbda)\n",
    "print('initial cost function J =', J)\n",
    "train_pred = predict(Theta1, Theta2, train)\n",
    "print('initial accuracy on training set =', np.sum(1.*(train_pred==train_label))/len(train_label))\n",
    "global Js_train\n",
    "global Js_test\n",
    "Js_train = np.array([J])\n",
    "J_test = cost_function(theta0, input_layer_size, hidden_layer_size, num_labels, test, test_label, lmbda)\n",
    "Js_test = np.array([J_test])\n",
    "\n",
    "# Minimize the cost function using a nonlinear conjugate gradient algorithm\n",
    "args = (input_layer_size, hidden_layer_size, num_labels, X, y, lmbda)  # parameter values\n",
    "# cbf = partial(callbackF, input_layer_size, hidden_layer_size, num_labels, X, y, lmbda, test, test_label)\n",
    "# theta = optimize.fmin_cg(cost_function, theta0, fprime=gradient, args=args, callback=cbf, maxiter=1)\n",
    "theta_best = optimize.fmin_cg(cost_function, theta0, fprime=gradient, args=args, maxiter=20)\n",
    "\n",
    "# unflatten theta\n",
    "Theta1, Theta2 = reshape(theta_best, input_layer_size, hidden_layer_size, num_labels)\n",
    "\n",
    "# Make predictions for the training and test sets\n",
    "train_pred = predict(Theta1, Theta2, train)\n",
    "test_pred = predict(Theta1, Theta2, test)\n",
    "\n",
    "# Print accuracy of predictions\n",
    "print('accuracy on training set =', np.sum(1.*(train_pred==train_label))/len(train_label))\n",
    "print('accuracy on test set =', np.sum(1.*(test_pred==test_label))/len(test_label))\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c60b646-c2d3-4287-9a5b-8fc780ae1920",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
