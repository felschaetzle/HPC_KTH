Timer unit: 1e-06 s

Total time: 0.491694 s
File: artificialneuralnetwork.py
Function: g at line 13

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    13                                           @profile
    14                                           def g(x):
    15                                           	""" sigmoid function """
    16    162130     491694.0      3.0    100.0  	return 1.0 / (1.0 + np.exp(-x))

Total time: 0.332923 s
File: artificialneuralnetwork.py
Function: grad_g at line 18

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    18                                           @profile
    19                                           def grad_g(x):
    20                                           	""" gradient of sigmoid function """
    21     54000     251421.0      4.7     75.5  	gx = g(x)
    22     54000      81502.0      1.5     24.5  	return gx * (1.0 - gx)	

Total time: 0.045497 s
File: artificialneuralnetwork.py
Function: predict at line 24

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    24                                           @profile
    25                                           def predict(Theta1, Theta2, X):
    26                                           	""" Predict labels in a trained three layer classification network.
    27                                           	Input:
    28                                           	  Theta1       trained weights applied to 1st layer (hidden_layer_size x input_layer_size+1)
    29                                           	  Theta2       trained weights applied to 2nd layer (num_labels x hidden_layer_size+1)
    30                                           	  X            matrix of training data      (m x input_layer_size)
    31                                           	Output:     
    32                                           	  prediction   label prediction
    33                                           	"""
    34                                           	
    35        23         92.0      4.0      0.2  	m = np.shape(X)[0]                    # number of training values
    36        23         42.0      1.8      0.1  	num_labels = np.shape(Theta2)[0]
    37                                           	
    38        23      30094.0   1308.4     66.1  	a1 = np.hstack((np.ones((m,1)), X))   # add bias (input layer)
    39        23      11822.0    514.0     26.0  	a2 = g(a1 @ Theta1.T)                 # apply sigmoid: input layer --> hidden layer
    40        23        947.0     41.2      2.1  	a2 = np.hstack((np.ones((m,1)), a2))  # add bias (hidden layer)
    41        23       1623.0     70.6      3.6  	a3 = g(a2 @ Theta2.T)                 # apply sigmoid: hidden layer --> output layer
    42                                           	
    43        23        865.0     37.6      1.9  	prediction = np.argmax(a3,1).reshape((m,1))
    44        23         12.0      0.5      0.0  	return prediction

Total time: 0.000313 s
File: artificialneuralnetwork.py
Function: reshape at line 47

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    47                                           @profile
    48                                           def reshape(theta, input_layer_size, hidden_layer_size, num_labels):
    49                                           	""" reshape theta into Theta1 and Theta2, the weights of our neural network """
    50        73         58.0      0.8     18.5  	ncut = hidden_layer_size * (input_layer_size+1)
    51        73        155.0      2.1     49.5  	Theta1 = theta[0:ncut].reshape(hidden_layer_size, input_layer_size+1)
    52        73         72.0      1.0     23.0  	Theta2 = theta[ncut:].reshape(num_labels, hidden_layer_size+1)
    53        73         28.0      0.4      8.9  	return Theta1, Theta2

Total time: 0.118299 s
File: artificialneuralnetwork.py
Function: cost_function at line 56

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    56                                           @profile
    57                                           def cost_function(theta, input_layer_size, hidden_layer_size, num_labels, X, y, lmbda):
    58                                           	""" Neural net cost function for a three layer classification network.
    59                                           	Input:
    60                                           	  theta               flattened vector of neural net model parameters
    61                                           	  input_layer_size    size of input layer
    62                                           	  hidden_layer_size   size of hidden layer
    63                                           	  num_labels          number of labels
    64                                           	  X                   matrix of training data
    65                                           	  y                   vector of training labels
    66                                           	  lmbda               regularization term
    67                                           	Output:
    68                                           	  J                   cost function
    69                                           	"""
    70                                           	
    71                                           	# unflatten theta
    72        42        349.0      8.3      0.3  	Theta1, Theta2 = reshape(theta, input_layer_size, hidden_layer_size, num_labels)
    73                                           	
    74                                           	# number of training values
    75        42         32.0      0.8      0.0  	m = len(y)
    76                                           	
    77                                           	# Feedforward: calculate the cost function J:
    78                                           	
    79        42      68474.0   1630.3     57.9  	a1 = np.hstack((np.ones((m,1)), X))   
    80        42      33486.0    797.3     28.3  	a2 = g(a1 @ Theta1.T)                 
    81        42       2057.0     49.0      1.7  	a2 = np.hstack((np.ones((m,1)), a2))  
    82        42       3616.0     86.1      3.1  	a3 = g(a2 @ Theta2.T)                 
    83                                           
    84        42       1417.0     33.7      1.2  	y_mtx = 1.*(y==0)
    85       126        169.0      1.3      0.1  	for k in range(1,num_labels):
    86        84       2261.0     26.9      1.9  		y_mtx = np.hstack((y_mtx, 1.*(y==k)))
    87                                           
    88                                           	# cost function
    89        42       5018.0    119.5      4.2  	J = np.sum( -y_mtx * np.log(a3) - (1.0-y_mtx) * np.log(1.0-a3) ) / m
    90                                           
    91                                           	# add regularization
    92        42       1396.0     33.2      1.2  	J += lmbda/(2.*m) * (np.sum(Theta1[:,1:]**2)  + np.sum(Theta2[:,1:]**2))
    93                                           	
    94        42         24.0      0.6      0.0  	return J

Total time: 3.11834 s
File: artificialneuralnetwork.py
Function: gradient at line 97

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    97                                           @profile
    98                                           def gradient(theta, input_layer_size, hidden_layer_size, num_labels, X, y, lmbda):
    99                                           	""" Neural net cost function gradient for a three layer classification network.
   100                                           	Input:
   101                                           	  theta               flattened vector of neural net model parameters
   102                                           	  input_layer_size    size of input layer
   103                                           	  hidden_layer_size   size of hidden layer
   104                                           	  num_labels          number of labels
   105                                           	  X                   matrix of training data
   106                                           	  y                   vector of training labels
   107                                           	  lmbda               regularization term
   108                                           	Output:
   109                                           	  grad                flattened vector of derivatives of the neural network 
   110                                           	"""
   111                                           	
   112                                           	# unflatten theta
   113        20        228.0     11.4      0.0  	Theta1, Theta2 = reshape(theta, input_layer_size, hidden_layer_size, num_labels)
   114                                           	
   115                                           	# number of training values
   116        20         15.0      0.8      0.0  	m = len(y)
   117                                           	
   118                                           	# Backpropagation: calculate the gradients Theta1_grad and Theta2_grad:
   119                                           	
   120        20         90.0      4.5      0.0  	Delta1 = np.zeros((hidden_layer_size,input_layer_size+1))
   121        20         24.0      1.2      0.0  	Delta2 = np.zeros((num_labels,hidden_layer_size+1))
   122                                           
   123     54020      21324.0      0.4      0.7  	for t in range(m):
   124                                           		
   125                                           		# forward
   126     54000      54137.0      1.0      1.7  		a1 = X[t,:].reshape((input_layer_size,1))
   127     54000     377022.0      7.0     12.1  		a1 = np.vstack((1, a1))   #  +bias
   128     54000     132027.0      2.4      4.2  		z2 = Theta1 @ a1
   129     54000     264357.0      4.9      8.5  		a2 = g(z2)
   130     54000     329494.0      6.1     10.6  		a2 = np.vstack((1, a2))   #  +bias
   131     54000     325142.0      6.0     10.4  		a3 = g(Theta2 @ a2)
   132                                           		
   133                                           		# compute error for layer 3
   134     54000      38587.0      0.7      1.2  		y_k = np.zeros((num_labels,1))
   135     54000      98948.0      1.8      3.2  		y_k[y[t,0].astype(int)] = 1
   136     54000      47281.0      0.9      1.5  		delta3 = a3 - y_k
   137     54000     110726.0      2.1      3.6  		Delta2 += (delta3 @ a2.T)
   138                                           		
   139                                           		# compute error for layer 2
   140     54000     559402.0     10.4     17.9  		delta2 = (Theta2[:,1:].T @ delta3) * grad_g(z2)
   141     54000     758422.0     14.0     24.3  		Delta1 += (delta2 @ a1.T)	
   142                                           
   143        20        133.0      6.7      0.0  	Theta1_grad = Delta1 / m
   144        20         30.0      1.5      0.0  	Theta2_grad = Delta2 / m
   145                                           
   146                                           	# add regularization
   147        20        672.0     33.6      0.0  	Theta1_grad[:,1:] += (lmbda/m) * Theta1[:,1:]	
   148        20        101.0      5.0      0.0  	Theta2_grad[:,1:] += (lmbda/m) * Theta2[:,1:]
   149                                           
   150                                           	# flatten gradients
   151        20        179.0      8.9      0.0  	grad = np.concatenate((Theta1_grad.flatten(), Theta2_grad.flatten()))
   152                                           
   153        20          2.0      0.1      0.0  	return grad

Total time: 0.082735 s
File: artificialneuralnetwork.py
Function: callbackF at line 162

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   162                                           @profile
   163                                           def callbackF(input_layer_size, hidden_layer_size, num_labels, X, y, lmbda, test, test_label, theta_k):
   164                                           	""" Calculate some stats per iteration and update plot """
   165        10          7.0      0.7      0.0  	plot = False
   166                                           
   167                                           	global N_iter
   168                                           	global J_min
   169                                           	global theta_best
   170                                           	global Js_train
   171                                           	global Js_test
   172                                           	# unflatten theta
   173        10         71.0      7.1      0.1  	Theta1, Theta2 = reshape(theta_k, input_layer_size, hidden_layer_size, num_labels)
   174                                           	# training data stats
   175        10      35855.0   3585.5     43.3  	J = cost_function(theta_k, input_layer_size, hidden_layer_size, num_labels, X, y, lmbda)
   176        10      29300.0   2930.0     35.4  	y_pred = predict(Theta1, Theta2, X)
   177        10        575.0     57.5      0.7  	accuracy = np.sum(1.*(y_pred==y))/len(y)
   178        10        238.0     23.8      0.3  	Js_train = np.append(Js_train, J)
   179                                           	# test data stats
   180        10       8891.0    889.1     10.7  	J_test = cost_function(theta_k, input_layer_size, hidden_layer_size, num_labels, test, test_label, lmbda)
   181        10       6855.0    685.5      8.3  	test_pred = predict(Theta1, Theta2, test)
   182        10        275.0     27.5      0.3  	accuracy_test = np.sum(1.*(test_pred==test_label))/len(test_label)
   183        10        167.0     16.7      0.2  	Js_test= np.append(Js_test, J_test)
   184                                           	# print stats
   185        10        432.0     43.2      0.5  	print('iter={:3d}:  Jtrain= {:0.4f} acc= {:0.2f}%  |  Jtest= {:0.4f} acc= {:0.2f}%'.format(N_iter, J, 100*accuracy, J_test, 100*accuracy_test))
   186        10         13.0      1.3      0.0  	N_iter += 1
   187                                           	# Update theta_best
   188        10         19.0      1.9      0.0  	if (J_test < J_min):
   189        10         16.0      1.6      0.0  		theta_best = theta_k
   190        10         10.0      1.0      0.0  		J_min = J_test
   191                                           
   192                                           	# Update Plot
   193        10         11.0      1.1      0.0  	if(plot): 
   194                                           		iters = np.arange(len(Js_train))
   195                                           		plt.clf()
   196                                           		plt.subplot(2,1,1)
   197                                           		im_size = 32
   198                                           		pad = 4
   199                                           		galaxies_image = np.zeros((3*im_size,6*im_size+2*pad), dtype=int) + 255
   200                                           		for i in range(3):
   201                                           			for j in range(6):
   202                                           				idx = 3*j + i + 900*(j>1) + 900*(j>3) + (N_iter % 600) # +10
   203                                           				shift = 0 + pad*(j>1) + pad*(j>3)
   204                                           				ii = i * im_size
   205                                           				jj = j * im_size + shift
   206                                           				galaxies_image[ii:ii+im_size,jj:jj+im_size] = X[idx].reshape(im_size,im_size) * 255
   207                                           				my_label = 'E' if y_pred[idx]==0 else 'S' if y_pred[idx]==1 else 'I'
   208                                           				my_color = 'blue' if (y_pred[idx]==y[idx]) else 'red'
   209                                           				plt.text(jj+2, ii+10, my_label, color=my_color)
   210                                           				if (y_pred[idx]==y[idx]):
   211                                           					plt.text(jj+4, ii+25, "✓", color='blue', fontsize=50)
   212                                           		
   213                                           		plt.imshow(galaxies_image, cmap='gray')
   214                                           		plt.gca().axis('off')
   215                                           		plt.subplot(2,1,2)
   216                                           		plt.plot(iters, Js_test, 'r', label='test')
   217                                           		plt.plot(iters, Js_train, 'b', label='train')
   218                                           		plt.xlabel("iteration")
   219                                           		plt.ylabel("cost")
   220                                           		plt.xlim(0,600)
   221                                           		plt.ylim(1,2.1)
   222                                           		plt.gca().legend()
   223                                           		plt.pause(0.001)

Total time: 12.0885 s
File: artificialneuralnetwork.py
Function: main at line 226

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   226                                           @profile
   227                                           def main():
   228                                           	""" Artificial Neural Network for classifying galaxies """
   229                                           	
   230                                           	# set the random number generator seed
   231         1         29.0     29.0      0.0  	np.random.seed(917)
   232                                           	
   233                                           	# Load the training and test datasets
   234         1    1346832.0 1346832.0     11.1  	train = np.genfromtxt('train.csv', delimiter=',')
   235         1     322805.0 322805.0      2.7  	test = np.genfromtxt('test.csv', delimiter=',')
   236                                           	
   237                                           	# get labels (0=Elliptical, 1=Spiral, 2=Irregular)
   238         1         14.0     14.0      0.0  	train_label = train[:,0].reshape(len(train),1)
   239         1          2.0      2.0      0.0  	test_label = test[:,0].reshape(len(test),1)
   240                                           	
   241                                           	# normalize image data to [0,1]
   242         1       5218.0   5218.0      0.0  	train = train[:,1:] / 255.
   243         1        802.0    802.0      0.0  	test = test[:,1:] / 255.
   244                                           	
   245                                           	# Construct our data matrix X (2700 x 5000)
   246         1          1.0      1.0      0.0  	X = train
   247                                           
   248                                               # Construct our label vector y (2700 x 1)
   249         1          0.0      0.0      0.0  	y = train_label
   250                                           	
   251                                           	# Two layer Neural Network parameters:
   252         1         11.0     11.0      0.0  	m = np.shape(X)[0]
   253         1          1.0      1.0      0.0  	input_layer_size = np.shape(X)[1]
   254         1          1.0      1.0      0.0  	hidden_layer_size = 8
   255         1          1.0      1.0      0.0  	num_labels = 3
   256         1          1.0      1.0      0.0  	lmbda = 1.0    # regularization parameter
   257                                           	
   258                                           	# Initialize random weights:
   259         1         68.0     68.0      0.0  	Theta1 = np.random.rand(hidden_layer_size, input_layer_size+1) * 0.4 - 0.2
   260         1          4.0      4.0      0.0  	Theta2 = np.random.rand(num_labels, hidden_layer_size+1) * 0.4 - 0.2
   261                                           	
   262                                           	# flattened initial guess
   263         1         20.0     20.0      0.0  	theta0 = np.concatenate((Theta1.flatten(), Theta2.flatten()))
   264         1       5976.0   5976.0      0.0  	J = cost_function(theta0, input_layer_size, hidden_layer_size, num_labels, X, y, lmbda)
   265         1         36.0     36.0      0.0  	print('initial cost function J =', J)
   266         1       5849.0   5849.0      0.0  	train_pred = predict(Theta1, Theta2, train)
   267         1         92.0     92.0      0.0  	print('initial accuracy on training set =', np.sum(1.*(train_pred==train_label))/len(train_label))
   268                                           	global Js_train
   269                                           	global Js_test
   270         1          6.0      6.0      0.0  	Js_train = np.array([J])
   271         1        907.0    907.0      0.0  	J_test = cost_function(theta0, input_layer_size, hidden_layer_size, num_labels, test, test_label, lmbda)
   272         1          4.0      4.0      0.0  	Js_test = np.array([J_test])
   273                                           
   274                                           	# prep figure
   275         1     135889.0 135889.0      1.1  	fig = plt.figure(figsize=(6,6), dpi=80)
   276                                           
   277                                           	# Minimize the cost function using a nonlinear conjugate gradient algorithm
   278         1          1.0      1.0      0.0  	args = (input_layer_size, hidden_layer_size, num_labels, X, y, lmbda)  # parameter values
   279         1          2.0      2.0      0.0  	cbf = partial(callbackF, input_layer_size, hidden_layer_size, num_labels, X, y, lmbda, test, test_label)
   280         1    3546124.0 3546124.0     29.3  	theta = optimize.fmin_cg(cost_function, theta0, fprime=gradient, args=args, callback=cbf, maxiter=10)
   281                                           
   282                                           	# unflatten theta
   283         1         14.0     14.0      0.0  	Theta1, Theta2 = reshape(theta_best, input_layer_size, hidden_layer_size, num_labels)
   284                                           	
   285                                           	# Make predictions for the training and test sets
   286         1       3000.0   3000.0      0.0  	train_pred = predict(Theta1, Theta2, train)
   287         1        726.0    726.0      0.0  	test_pred = predict(Theta1, Theta2, test)
   288                                           	
   289                                           	# Print accuracy of predictions
   290         1         85.0     85.0      0.0  	print('accuracy on training set =', np.sum(1.*(train_pred==train_label))/len(train_label))
   291         1         27.0     27.0      0.0  	print('accuracy on test set =', np.sum(1.*(test_pred==test_label))/len(test_label))	
   292                                           			
   293                                           	# Save figure
   294         1      56275.0  56275.0      0.5  	plt.savefig('artificialneuralnetwork.png',dpi=240)
   295         1    6657685.0 6657685.0     55.1  	plt.show()
   296                                           	    
   297         1          3.0      3.0      0.0  	return 0
